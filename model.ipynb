{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# from sklearn.svm import OneClassSVM\n"
      ],
      "metadata": {
        "id": "2u2y8olOezCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: ProtBERT Tokenizer and Embedding Generation\n",
        "# ----------------------------------------\n",
        "# Load ProtBERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
        "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
        "\n",
        "# Example peptide sequences\n",
        "peptides = [\"AAGWDF\", \"VVKYPQ\", \"GKLSHF\"]  # Replace with your actual peptides\n",
        "formatted_peptides = [\" \".join(list(seq)) for seq in peptides]\n",
        "\n",
        "# Tokenize and get embeddings\n",
        "inputs = tokenizer(formatted_peptides, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the last hidden state (embeddings)\n",
        "sequence_embeddings = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)\n",
        "sequence_embeddings_avg = sequence_embeddings.mean(dim=1).numpy()  # Shape: (batch_size, hidden_dim)\n",
        "\n",
        "print(f\"Shape of ProtBERT embeddings: {sequence_embeddings_avg.shape}\")\n"
      ],
      "metadata": {
        "id": "-PGKpLFBe8ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Auxiliary Data\n",
        "# ----------------------------------------\n",
        "# Example auxiliary data (e.g., structural/functional info), set missing entries to 0\n",
        "auxiliary_data = np.array([[0.1, 0.2, 0.3], [0.4, 0.6, 0.8], [0.5, 0.1, 0.0]])  # Shape: (batch_size, aux_dim)\n",
        "auxiliary_data = torch.tensor(auxiliary_data, dtype=torch.float32)\n",
        "\n",
        "# Step 3: One-Class random forest on ProtBERT Embeddings\n",
        "# ----------------------------------------\n",
        "# Train One-Class RF\n",
        "\n",
        "# Assuming `sequence_embeddings_avg` contains embeddings of your positive peptides\n",
        "# Train Isolation Forest on the positive embeddings\n",
        "one_class_rf = IsolationForest(contamination=0.1)  # Set contamination as per your expected outliers\n",
        "one_class_rf.fit(sequence_embeddings_avg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Predict bioactivity (SVM predicts +1 for inliers, -1 for outliers)\n",
        "rf_predictions = one_class_rf.predict(sequence_embeddings_avg)\n",
        "print(f\"RF predictions: {rf_predictions}\")  # 1 for likely bioactive, 0 for non-bioactive\n",
        "\n"
      ],
      "metadata": {
        "id": "CjIGoON4e__R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: N1-NN Neural Network for Bioactivity Prediction with Auxiliary Data\n",
        "# ----------------------------------------\n",
        "class N1NN_BioactivePeptideClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, aux_dim, rf_dim, hidden_dim):\n",
        "        super(N1NN_BioactivePeptideClassifier, self).__init__()\n",
        "\n",
        "        self.fc1_seq = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc1_aux = nn.Linear(aux_dim, hidden_dim)\n",
        "        self.fc1_rf = nn.Linear(rf_dim, hidden_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim * 3, hidden_dim)  # Three inputs\n",
        "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x_seq, x_aux=None, x_rf=None):\n",
        "        # Peptide sequence path\n",
        "        x_seq = torch.relu(self.fc1_seq(x_seq))\n",
        "\n",
        "        # Auxiliary data path\n",
        "        x_aux = torch.relu(self.fc1_aux(x_aux))\n",
        "\n",
        "        # SVM features path\n",
        "        x_rf = torch.relu(self.fc1_rf(x_rf))\n",
        "\n",
        "        # Concatenate all features\n",
        "        x_combined = torch.cat((x_seq, x_aux, x_rf), dim=1)\n",
        "\n",
        "        # Fully connected layers for final prediction\n",
        "        x_combined = torch.relu(self.fc2(x_combined))\n",
        "        output = torch.sigmoid(self.output_layer(x_combined))\n",
        "        return output\n",
        "\n",
        "# Instantiate the N1-NN model\n",
        "n1nn_model = N1NN_BioactivePeptideClassifier(input_dim=sequence_embeddings_avg.shape[1],\n",
        "                                             aux_dim=auxiliary_data.shape[1],\n",
        "                                             rf_dim=1,  # SVM prediction is a single feature\n",
        "                                             hidden_dim=64)\n",
        "\n"
      ],
      "metadata": {
        "id": "cINrRFkIfCg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train N1-NN Model with ProtBERT Embeddings, Auxiliary Data, and SVM Features\n",
        "# ----------------------------------------\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy for bioactivity prediction\n",
        "optimizer = torch.optim.Adam(n1nn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Example training labels (since you have only bioactive peptides, use 1s for now)\n",
        "labels = torch.tensor([1, 1, 1], dtype=torch.float32)  # Replace with your actual labels (if available)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Increase epochs for more training\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Convert ProtBERT embeddings back to tensor\n",
        "    inputs_tensor = torch.tensor(sequence_embeddings_avg, dtype=torch.float32)\n",
        "\n",
        "    # Forward pass through N1-NN with embeddings, auxiliary data, and SVM features\n",
        "    outputs = n1nn_model(inputs_tensor, auxiliary_data, rf_features)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "glIY312ZfFFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Inference and Prediction using SVM and N1-NN\n",
        "# ----------------------------------------\n",
        "def predict_bioactivity(peptide_sequence):\n",
        "    # Tokenize and get embedding for the input peptide sequence\n",
        "    formatted_peptide = \" \".join(list(peptide_sequence))\n",
        "    inputs = tokenizer([formatted_peptide], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the average embedding for the sequence\n",
        "    embedding_avg = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "    # Step 1: Use One-Class SVM to determine if the peptide is bioactive\n",
        "    rf_pred = one_class_rf.predict(embedding_avg)\n",
        "\n",
        "    # Convert SVM prediction to tensor for input into N1-NN\n",
        "    rf_feature = torch.tensor((svm_pred == 1).astype(np.float32)).unsqueeze(1)  # Shape: (1, 1)\n",
        "\n",
        "    # Step 2: If the SVM classifies it as bioactive, pass through the N1-NN\n",
        "    embedding_tensor = torch.tensor(embedding_avg, dtype=torch.float32)\n",
        "\n",
        "    # Create auxiliary feature tensor (set to zero for inference)\n",
        "    auxiliary_tensor = torch.zeros((1, auxiliary_data.shape[1]), dtype=torch.float32)  # Shape: (1, aux_dim)\n",
        "\n",
        "    bioactivity_prob = n1nn_model(embedding_tensor, auxiliary_tensor, rf_feature).item()\n",
        "\n",
        "    return f\"Bioactive (Probability: {bioactivity_prob:.4f})\"\n",
        "\n",
        "# Example prediction on a new peptide sequence\n",
        "new_peptide = \"MVKVYAPASSANMSVGFDVLGAAVTPVDGALLGDVVTVEAAETFSLNNLGQKAVKDY\"\n",
        "bioactivity_prediction = predict_bioactivity(new_peptide)\n",
        "print(f\"Prediction for new peptide: {bioactivity_prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrU8LDkBesqY",
        "outputId": "dc2f82ab-1090-4752-e262-29122750eb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8maJB_AfHXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}